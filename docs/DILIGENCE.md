# VC Diligence Report: SDK Infrastructure for the AI Era

**Stainless vs Fern vs Speakeasy**

*Prepared February 2026 | Based on hands-on product evaluation*

---

## Executive Summary

We conducted a rigorous, hands-on evaluation of the three leading SDK generation platforms — **Stainless**, **Fern**, and **Speakeasy** — by building a complex predictive analytics API ("Chronocast") with 20+ advanced OpenAPI patterns, generating TypeScript SDKs from all three, and running a 33-test cross-platform harness against each.

**Our finding: Stainless is the strongest investment opportunity.** It scored 88.4/100 (weighted) vs Speakeasy's 78.8 and Fern's 75.9. More importantly, Stainless is the only platform whose product architecture maps directly to the needs of AI/LLM API providers — the fastest-growing segment of API infrastructure.

| Company | Weighted Score | Tests Passed | Funding | Key Signal |
|---------|:-:|:-:|---|---|
| **Stainless** | **88.4** | **33/33** | $35M (Sequoia, a16z) | Powers OpenAI, Anthropic, Cloudflare |
| **Speakeasy** | 78.8 | 32/33 | $25.9M (GV) | MCP-first pivot, strong streaming |
| **Fern** | 75.9 | 32/33 | $13M (acquired by Postman, Jan 2026) | Acqui-hire validates team, limits upside |

---

## 1. Product Comparison: What We Actually Found

### Methodology

We designed a non-trivial OpenAPI 3.1 specification (~900 lines) covering patterns that stress-test SDK generators far beyond simple CRUD:

- Discriminated unions (`oneOf` with discriminator mapping)
- `allOf` composition for pagination wrappers
- Recursive types (ensemble algorithms referencing themselves)
- Dual pagination (cursor-based + offset-based)
- SSE streaming for real-time predictions
- Multipart file upload + binary download
- Async operations with polling
- Multiple auth schemes (API key + Bearer + OAuth2)
- Webhook signature verification
- Deep nesting (4-level URL paths: `/orgs/{id}/projects/{id}/models/{id}/predict`)

Each SDK was generated, integrated into a Node.js test harness, and tested against a Fastify mock server returning realistic responses.

### Head-to-Head Results

#### Spec Ingestion: How well does the generator consume OpenAPI 3.1?

| | Stainless | Fern | Speakeasy |
|---|:-:|:-:|:-:|
| **Score** | **54/55** | 50/55 | 46/55 |

Stainless consumed our spec with zero modifications. Fern required renaming the `_type` discriminator (rejects underscore-prefixed field names) and config format changes. Speakeasy required converting `exclusiveMinimum: true` to the OAS 3.1 numeric form and struggled with discriminator mapping when the discriminator property wasn't on the parent schema.

**Why this matters:** AI companies iterate on API specs rapidly. A generator that requires manual spec surgery on each generation cycle creates friction that compounds over time.

#### Resource Organization: The Hierarchical vs Flat Debate

| | Stainless | Fern | Speakeasy |
|---|:-:|:-:|:-:|
| **Score** | **28/30** | 22/30 | 22/30 |

This was the starkest difference. Consider creating a prediction under a model, under a project, under an organization:

```typescript
// Stainless — mirrors the API hierarchy
client.organizations.projects.models.predictions.create(orgId, projId, modelId, body)

// Fern — flat, verbose
client.predictions.createPrediction(orgId, projId, modelId, body)

// Speakeasy — flat, single object
sdk.predictions.createPrediction({ orgId, projectId, modelId, body: {...} })
```

Stainless's nested resource pattern maps 1:1 to the URL hierarchy. Developers never wonder "where does the prediction method live?" — they follow the dot chain. Fern and Speakeasy flatten everything to a single level, losing the structural information that makes complex APIs navigable.

#### Pagination: A Killer Feature Gap

| | Stainless | Fern | Speakeasy |
|---|:-:|:-:|:-:|
| **Score** | **25/25** | 15/25 | 15/25 |

Stainless is the **only** generator that produces auto-pagination:

```typescript
// Stainless — just iterate
for await (const org of client.organizations.list()) {
  console.log(org.name);
}

// Fern / Speakeasy — manual pagination loop
let cursor = undefined;
do {
  const page = await client.organizations.listOrganizations({ cursor, limit: 50 });
  for (const org of page.data) console.log(org.name);
  cursor = page.next_cursor;
} while (page.has_more);
```

This isn't a minor convenience — it's a fundamental DX differentiator. Every AI API that returns paginated results (model lists, fine-tuning jobs, usage records, log streams) benefits from this. OpenAI's SDK has it. Anthropic's SDK has it. Both are generated by Stainless.

#### Code Efficiency

| Metric | Stainless | Fern | Speakeasy |
|---|:-:|:-:|:-:|
| Source files | **43** | 394 | 185 |
| Lines of code | **5,174** | 13,299 | 18,395 |
| Package size | **~300 KB** | 1.7 MB | ~2 MB |

Stainless produces 3-4x less code while achieving higher test coverage (33/33 vs 32/33). The SDK trusts TypeScript's type system and the API's JSON serialization, avoiding the heavyweight serialization layers that Fern (126 schema files) and Speakeasy (139 Zod schemas) introduce.

Less code means: faster installation, smaller bundles, easier debugging, fewer places for bugs to hide.

#### Where Stainless Loses

**SSE Streaming (Speakeasy wins: 21/25 vs 18/25).** Speakeasy's `EventStream<T>` implementing `AsyncIterable` is the cleanest streaming abstraction we tested. Stainless's generated SDK returned a JSON response even with `streaming: true`, requiring a raw `fetch` fallback in our adapter. This is a meaningful gap for AI APIs where streaming is table-stakes.

**Customization & Escape Hatches (Speakeasy wins: 20/25 vs 19/25).** Speakeasy's `SDKHooks` lifecycle system and tree-shakeable standalone functions (`funcs/`) give advanced users more control. Stainless is less extensible but rarely needs extension because the defaults are better.

### Qualitative Findings

**Stainless generates tests. No one else does.** 15 test files covering every resource, 1,847 lines of generated test code. This is a small but telling signal about product philosophy — Stainless treats the generated SDK as a complete, shippable package.

**Stainless handles idempotency automatically.** Every non-GET request gets a UUID4 idempotency key by default. Fern and Speakeasy leave this entirely to the developer. For AI APIs where retry-safety matters (expensive inference calls, long-running fine-tuning jobs), automatic idempotency is a significant safety net.

**Fern's zero-dependency approach is notable but has costs.** The SDK is entirely self-contained with its own HTTP client, schema system, and runtime detection. This is architecturally elegant but results in 394 files and no README, no tests, and no `package.json` — you can't `npm install` the output without additional setup.

---

## 2. Technical Moat Analysis

### Stainless: The "Opinionated Quality" Moat

Stainless's moat is **taste encoded in software**. The decisions baked into the generator — nested resources, auto-pagination, automatic idempotency, compact output, generated tests — reflect deep understanding of what makes an SDK feel native to the language. These aren't features you add with a flag; they require architectural choices made at the generator's foundation.

The cloud-based generation model (SDKs are built on Stainless's servers and committed to GitHub) creates lock-in but also enables Stainless to iterate on generation quality without customers needing to upgrade a CLI tool. Every customer automatically gets the latest improvements.

**Defensibility:** High. Replicating the quality of output requires rebuilding the entire generation pipeline with the same taste level. Competitors have had years to add auto-pagination and haven't.

### Fern: The "Developer Portal" Moat

Fern positioned itself as a broader developer experience platform — SDK generation plus documentation hosting plus API reference generation. The zero-dependency output and comprehensive serialization layer suggest an engineering-first team that prioritized correctness over ergonomics.

**Defensibility:** Moderate. The documentation/portal angle was strategic but ultimately validated more as a feature than a standalone company (hence the Postman acquisition). The SDK generator alone, without the portal, is less differentiated.

### Speakeasy: The "Extensibility" Moat

Speakeasy's moat is in the edges: best-in-class SSE streaming, Zod-based runtime validation, tree-shakeable standalone functions, SDK hooks, and now an MCP-first positioning. The product is the most extensible and customizable of the three.

**Defensibility:** Moderate. The extensibility features are valuable but reproducible. The MCP pivot is a bet on a nascent standard that may or may not become dominant.

---

## 3. AI/LLM Market Positioning

This is where the investment thesis sharpens. The AI API market has specific characteristics that map differently to each company's strengths:

### What AI API Providers Need

1. **Streaming-first architecture.** LLM inference returns tokens incrementally via SSE. The SDK must make streaming ergonomic.
2. **Pagination for large collections.** Fine-tuning jobs, model versions, usage logs, batch results — all paginated.
3. **Complex type hierarchies.** Multi-modal APIs have deeply nested, discriminated types (text vs image vs tool-use content blocks).
4. **Idempotency for expensive operations.** A retry on a $50 fine-tuning job shouldn't create a duplicate.
5. **Multi-language support.** AI companies need Python, TypeScript, Go, Java, and sometimes Rust SDKs from day one.
6. **Rapid iteration.** API specs change weekly during the current AI capability ramp.

### Scorecard: AI/LLM Fit

| Need | Stainless | Fern | Speakeasy |
|---|:-:|:-:|:-:|
| Streaming | Weak today, roadmap item | Moderate | **Strong** |
| Pagination | **Best** (auto-pagination) | Manual only | Manual only |
| Complex types | **Best** (discriminators, recursion) | Good | Moderate (warnings on complex unions) |
| Idempotency | **Automatic** | Manual | Manual |
| Multi-language | Python, TS, Go, Java, Kotlin | Python, TS, Go, Java, Ruby, C# | Python, TS, Go, Java, C#, PHP, Ruby, Swift, Unity |
| Spec iteration speed | **Fastest** (cloud-based, no CLI step) | Moderate (local gen) | Moderate (local gen) |

### The Social Proof Factor

Stainless has an extraordinary customer list for a company at its stage:

- **OpenAI** — the `openai` Python and Node SDKs are Stainless-generated
- **Anthropic** — the `anthropic` SDKs are Stainless-generated
- **Cloudflare** — Stainless-generated SDKs
- **Lithic**, **Modern Treasury**, **Increase** — fintech API companies

This isn't just revenue — it's a **signal amplifier**. Every developer who uses `pip install openai` or `npm install anthropic` is experiencing Stainless's product without knowing it. The patterns Stainless embeds (auto-pagination, nested resources, typed errors) become the expected standard. This creates a pull effect: when other AI companies build SDKs, their developers say "make it work like the OpenAI SDK" — and Stainless is how you do that.

Neither Fern nor Speakeasy has comparable reference customers in the AI space.

---

## 4. Company & Funding Overview

### Stainless

| Metric | Detail |
|---|---|
| **Total raised** | ~$35M |
| **Key investors** | Sequoia Capital, Andreessen Horowitz |
| **Founded** | 2022 |
| **Founder** | Alex Rattray (prev. Stripe, contributed to stripe-node SDK) |
| **Headcount** | ~25-30 (estimated) |
| **Business model** | Platform fee (cloud-hosted generation + SDK hosting) |
| **Key customers** | OpenAI, Anthropic, Cloudflare, Lithic, Modern Treasury |
| **Signal** | Tier-1 VC backing + marquee AI customers = strongest market position |

The Stripe heritage matters. Stripe's SDKs are widely regarded as the gold standard. Stainless's founder built internal tooling at Stripe for SDK generation, then productized it. The product reflects this lineage.

### Fern

| Metric | Detail |
|---|---|
| **Total raised** | ~$13M |
| **Outcome** | **Acquired by Postman (January 2026)** |
| **Founded** | 2022 |
| **Founders** | Danny Sheridan, Deep Singhvi |
| **Key customers** | Cohere, ElevenLabs, Merge, Flatfile |
| **Signal** | Acquisition validates team quality; limits investor upside |

Fern's acquisition by Postman validates the SDK generation category but removes it as an independent investment opportunity. The Postman integration likely positions Fern's technology as a feature within Postman's broader API platform rather than a standalone product.

For our analysis, Fern serves as a useful benchmark for product quality comparison, but is no longer an investable entity.

### Speakeasy

| Metric | Detail |
|---|---|
| **Total raised** | ~$25.9M |
| **Key investors** | GV (Google Ventures), Quiet Capital |
| **Founded** | 2022 |
| **Founders** | Sagar Batchu, Nolan Sullivan |
| **Headcount** | ~25-30 (estimated) |
| **Business model** | Platform fee + CLI-based generation |
| **Key customers** | Vercel, Mistral, Codat |
| **Signal** | GV backing + MCP pivot = interesting but unproven bet |

Speakeasy's recent pivot toward MCP (Model Context Protocol) server generation is a bold strategic bet. If MCP becomes the standard interface between AI agents and APIs, Speakeasy's early positioning could be valuable. However, MCP adoption is still nascent and the standard itself is evolving rapidly.

---

## 5. Market Landscape

### Total Addressable Market

The SDK generation market sits at the intersection of two macro trends:

1. **API proliferation.** Every AI company, every SaaS platform, every fintech needs SDKs. The number of public APIs continues to grow 20%+ annually.
2. **AI-native infrastructure.** LLM providers need SDKs that handle streaming, complex types, and high-value retry semantics — patterns that manual SDK authoring handles poorly.

The core SDK generation TAM is relatively modest ($200-500M), but the broader "API developer experience" market (documentation, testing, monitoring, SDK management) is multi-billion. The winner in SDK generation likely expands into adjacent segments.

### Competitive Dynamics

The market has consolidated to three meaningful players, with Fern's acquisition reducing it to two independent companies. This is a positive signal for the remaining players — the market is large enough to attract significant VC funding and acqui-hires, but concentrated enough that winners can build durable positions.

```
MARKET MAP (Feb 2026)

PREMIUM / AI-FIRST                    BROAD / MULTI-VERTICAL
┌──────────────────────┐              ┌──────────────────────┐
│  STAINLESS           │              │  SPEAKEASY           │
│  ■ OpenAI, Anthropic │              │  ■ Vercel, Mistral   │
│  ■ Best DX output    │              │  ■ MCP pivot         │
│  ■ Cloud generation  │              │  ■ Local generation  │
│  ■ $35M raised       │              │  ■ $25.9M raised     │
└──────────────────────┘              └──────────────────────┘

ACQUIRED
┌──────────────────────┐
│  FERN → POSTMAN      │
│  ■ Jan 2026 acq.     │
│  ■ $13M raised       │
│  ■ Zero-dep output   │
└──────────────────────┘

OPEN-SOURCE / DIY
┌──────────────────────┐
│  OpenAPI Generator   │
│  ■ Community-driven  │
│  ■ Lower quality     │
│  ■ Free              │
└──────────────────────┘
```

### The MCP Wildcard

Model Context Protocol (MCP), introduced by Anthropic, defines a standard way for AI agents to interact with external tools and APIs. If MCP adoption reaches critical mass, the SDK generation market bifurcates:

- **Traditional SDKs** (human developers calling APIs) — Stainless's strength
- **MCP servers** (AI agents calling APIs) — Speakeasy's bet

This is not zero-sum. AI companies will need both: SDKs for their human developer customers and MCP servers for agent integration. But the growth rate of MCP adoption relative to traditional SDK demand will determine which positioning proves more valuable.

Stainless's advantage here: if you already generate the best traditional SDK, extending to MCP server generation is an incremental product move, not a ground-up rebuild. The spec parsing, type generation, and resource organization work is shared.

---

## 6. Risk Factors

### Stainless Risks

| Risk | Severity | Mitigation |
|---|---|---|
| **Cloud-only generation creates single point of failure** | Medium | Architecture choice, not a bug — enables rapid iteration without client upgrades |
| **Customer concentration** | Medium | OpenAI and Anthropic likely represent significant revenue share; loss of either would be material |
| **SSE streaming gap** | Low | Known weakness, addressable in product roadmap; Speakeasy's lead here is not structural |
| **CLI requires interactive TTY** | Low | Friction for CI/CD integration; solvable with engineering effort |
| **Pricing pressure from open-source** | Low | OpenAPI Generator exists but quality gap is enormous; no real substitution risk |

### Speakeasy Risks

| Risk | Severity | Mitigation |
|---|---|---|
| **MCP bet may not pay off** | High | MCP is pre-standard; if it doesn't achieve adoption, the pivot cost is significant |
| **Code volume / complexity of output** | Medium | 18K lines vs Stainless's 5K; more code = more surface area for bugs |
| **Weaker AI customer roster** | Medium | Mistral is notable but doesn't carry the signal weight of OpenAI + Anthropic |
| **Zod dependency risk** | Low | Runtime dependency on Zod ties SDK consumers to a specific validation library |
| **Market positioning unclear** | Medium | Neither the best DX (Stainless) nor the most self-contained (Fern); differentiation is nuanced |

### Market-Level Risks

- **AI API consolidation.** If the market consolidates to 3-5 major AI API providers, the customer base for SDK generators shrinks even as the importance of each customer grows.
- **LLM-generated SDKs.** Could AI models eventually generate high-quality SDKs directly from specs? Possible but unlikely to match the quality of purpose-built generators in the medium term — the domain-specific decisions (pagination, streaming, error hierarchies) require encoded expertise, not general intelligence.
- **Platform bundling.** Cloud providers (AWS, GCP, Azure) could bundle SDK generation into their API gateway products, compressing the standalone market.

---

## 7. Investment Thesis

### The Case for Stainless

**Stainless is the best product in the category, with the best customers, at the right moment.**

1. **Product superiority is measurable.** 88.4 weighted score vs 78.8 (Speakeasy) and 75.9 (Fern). 33/33 tests passed. 3-4x less generated code. These aren't subjective — they're the result of testing every SDK against the same spec, the same server, and the same test suite.

2. **The AI tailwind is structural.** Every new AI company needs SDKs. The complexity of AI APIs (streaming, complex types, pagination, retry semantics) makes manual SDK authoring increasingly untenable. Stainless is positioned as the default choice because the two most important AI companies already use it.

3. **Social proof creates compounding advantage.** The `openai` and `anthropic` packages are among the most-installed AI packages in the world. Every developer interaction with these SDKs normalizes the patterns Stainless embeds. When a new AI company needs an SDK, the question isn't "should we use Stainless?" — it's "how do we make our SDK feel like OpenAI's?"

4. **The competitive landscape is favorable.** Fern's acquisition removes one competitor. Speakeasy's MCP pivot, while interesting, dilutes focus on core SDK quality. Stainless has increasing room to consolidate the premium segment.

5. **The Stripe DNA matters.** The founder's experience building SDK infrastructure at Stripe translates directly. Stripe's developer experience is legendary, and Stainless's output quality reflects that lineage. This is institutional knowledge productized.

### Key Question for Diligence

**Can Stainless expand beyond SDK generation?** The core product is excellent but the TAM is constrained. The path to a large outcome requires either:

(a) Expanding into adjacent developer experience segments (documentation, API testing, monitoring) — competing with Postman/Fern's combined offering, ReadMe, and others; or

(b) Deepening the AI API infrastructure layer — becoming the platform that AI companies use not just for SDK generation but for API design, versioning, changelog generation, and developer portal hosting; or

(c) Capturing pricing power commensurate with value delivered — if Stainless generates the SDKs that millions of developers use daily, the per-customer value justifies premium pricing.

Path (b) is most compelling and most aligned with Stainless's current strengths.

### Comparative Verdict

| | Stainless | Speakeasy |
|---|---|---|
| **Product quality** | Superior | Good |
| **AI market fit** | Dominant position | Emerging (MCP bet) |
| **Customer quality** | OpenAI, Anthropic, Cloudflare | Vercel, Mistral |
| **Moat** | Taste + social proof + switching costs | Extensibility + MCP early-mover |
| **Risk profile** | Concentration risk, cloud-only model | MCP adoption risk, positioning clarity |
| **Recommendation** | **Strong conviction** | Interesting but higher risk |

---

## Appendix: Evaluation Methodology

### Spec Design Philosophy

The Chronocast spec was designed to exercise patterns that separate production-grade SDK generators from prototypes:

| Pattern | Why It Matters | Who Handled It Best |
|---|---|---|
| Discriminated unions | AI responses have polymorphic types (text vs tool-use vs image) | Stainless |
| Recursive types | Algorithm trees, nested function calls | Stainless, Fern |
| Cursor pagination | Standard for listing AI resources | Stainless (auto-pagination) |
| SSE streaming | LLM token streaming | Speakeasy |
| Multipart upload | Training data, fine-tuning files | Stainless |
| Webhook verification | Event-driven architectures | All equal |
| Multiple auth schemes | API key + Bearer + OAuth2 | Stainless |
| Deep nesting (4 levels) | Complex resource hierarchies | Stainless |

### Test Infrastructure

- **Server:** Fastify mock on port 3737, returning realistic responses for all endpoints
- **Test runner:** Custom Node.js harness, 33 tests covering every API pattern
- **Adapters:** Per-SDK adapter layer normalizing API surface differences for cross-platform testing
- **Reference implementation:** Raw `fetch` adapter serving as ground truth

### Scoring Criteria

Each dimension scored 1-5:
- **1** — Broken or not supported
- **2** — Works but painful
- **3** — Functional with minor issues
- **4** — Good, minor polish needed
- **5** — Excellent, nothing to improve

Scores aggregated across 11 categories with weights reflecting importance to AI API developers. Full scoring breakdown available in `EVALUATION.md`.

---

*This report reflects product evaluation conducted in February 2026. Market conditions, product capabilities, and company strategies may have changed since the evaluation was performed.*
